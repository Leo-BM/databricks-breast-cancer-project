{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75ea7e01-8f97-405d-bc5f-e513754eefc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "table_path = \"workspace.breast_cancer.breast_cancer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ce3f7d-8220-4897-b323-d0c759eaa8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.table(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340ba5c7-b6b2-4549-9e96-7cf0f6b4c6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_raw)\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14a20bb8-803f-4163-9177-4422e6c9c3ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df_clean = df_raw.drop(\"id\") \\\n",
    "    .withColumn(\n",
    "        \"label\",\n",
    "        F.when(F.col(\"diagnosis\") == \"M\", 1).otherwise(0)\n",
    "    ) \\\n",
    "    .drop(\"diagnosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8509c6-4f77-4d23-8627-60791defe146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_clean)\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b064e905-89c5-462b-be05-1e282cc4247b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "expressoes_agregacao = [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df_clean.columns]\n",
    "\n",
    "df_clean.select(expressoes_agregacao).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c277bb91-0a2a-417b-9b08-1a852788ce5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "N√£o h√° valores nulos no dataframe, podemos dar continuidade ao estudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f88255ba-5337-41ae-bb38-2a4d463f02c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols_to_compare = [\"radius_mean\", \"area_mean\", \"concavity_mean\", \"label\"]\n",
    "df_clean.select(cols_to_compare).summary(\"min\", \"max\", \"mean\", \"stddev\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d5bdf7-c567-4f30-9a36-1b6b3e637c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGZfZ3JvdXBlZCA9IGRmX2NsZWFuLmdyb3VwQnkoImxhYmVsIikuY291bnQoKQpkaXNwbGF5KGRmX2dyb3VwZWQp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView81d19bc\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView81d19bc\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView81d19bc\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView81d19bc) SELECT `label`,SUM(`count`) `column_2e4946fb78` FROM q GROUP BY `label`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView81d19bc\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "label",
             "id": "column_2e4946fb77"
            },
            "y": [
             {
              "column": "count",
              "id": "column_2e4946fb78",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "pie",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_2e4946fb78": {
             "name": "count",
             "type": "pie",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "23184baf-2fdf-4049-bae1-227be108f8ce",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "label",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "label",
           "type": "column"
          },
          {
           "alias": "column_2e4946fb78",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_grouped = df_clean.groupBy(\"label\").count()\n",
    "display(df_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b0430e1-d5b7-4db1-bd8b-ce1a46fb0597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"Conforme a teoria de SVM, identifiquei disparidade de escalas (ex: area_mean vs smoothness_mean). Isso torna o passo de Standard Scaling obrigat√≥rio para evitar o vi√©s do modelo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "464b3b58-8232-4c40-80ee-3116c16abeaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#separa√ß√£o das colunas num√©ricas para o pipeline\n",
    "feature_cols = [c for c in df_clean.columns if c != 'label']\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e91744ba-4978-41a7-ad3f-2d50c30d8539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=True\n",
    ")\n",
    "pipeline_preparacao = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "modelo_preparacao = pipeline_preparacao.fit(df_clean)\n",
    "\n",
    "df_final = modelo_preparacao.transform(df_clean)\n",
    "\n",
    "display(\n",
    "    df_final.select(\n",
    "        \"features_raw\",\n",
    "        \"features\",\n",
    "        \"label\"\n",
    "    ).limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924d77f5-3ca9-45f1-a585-789b500d19a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Divis√£o dos dados em 70% Treino e 30% Teste\n",
    "train_data, test_data = df_final.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"Registros para Treinamento: {train_data.count()}\")\n",
    "print(f\"Registros para Teste (Prova Real): {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17aa7045-d668-4f7e-a241-851f2227b315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  TREINAMENTO DO MODELO SVM\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "svm = LinearSVC(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",        \n",
    "    maxIter=100, \n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "svm_model = svm.fit(train_data)\n",
    "\n",
    "print(f\"Intercepto (b): {svm_model.intercept}\")\n",
    "print(f\"N√∫mero de pesos (w) aprendidos: {len(svm_model.coefficients)}\")\n",
    "\n",
    "\n",
    "predictions = svm_model.transform(test_data)\n",
    "\n",
    "# Visualizando o Resultado\n",
    "print(\"--- Resultados nos Dados de Teste ---\")\n",
    "display(predictions.select(\"label\", \"rawPrediction\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62ce1824-bfce-47e0-ac00-2dc77a55de1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  AVALIA√á√ÉO DO MODELO ---\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "evaluator_acc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_acc.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(f\"üéØ Acur√°cia Global SVC: {accuracy:.2%}\")\n",
    "print(f\"‚öñÔ∏è  F1-Score (Equil√≠brio) SVC: {f1_score:.2%}\")\n",
    "\n",
    "# Matriz de Confus√£o \n",
    "print(\"\\n--- Matriz de Confus√£o ---\")\n",
    "confusion_matrix = predictions.groupBy(\"label\", \"prediction\").count() \\\n",
    "                              .orderBy(\"label\", \"prediction\")\n",
    "\n",
    "display(confusion_matrix)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "#Calculando Recall e Precision para a classe 'Maligno' (1)\n",
    "TP = predictions.filter((F.col(\"label\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "FN = predictions.filter((F.col(\"label\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "FP = predictions.filter((F.col(\"label\") == 0) & (F.col(\"prediction\") == 1)).count()\n",
    "\n",
    "recall_maligno = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "precision_maligno = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üìä DETALHAMENTO DA CLASSE 'MALIGNO' (1.0)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"üö® Recall (Sensibilidade) SVC: {recall_maligno:.2%}\")\n",
    "print(f\"üéØ Precision (Precis√£o) SVC :   {precision_maligno:.2%} \")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc11b53-05e2-4015-983c-8593d6c3eefc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc9889e9-8401-4a56-918b-3993325d3eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop 'features_raw' from train_data and test_data if it exists para recalcular aqui\n",
    "if 'features_raw' in train_data.columns:\n",
    "    train_data = train_data.drop('features_raw')\n",
    "if 'features_raw' in test_data.columns:\n",
    "    test_data = test_data.drop('features_raw')\n",
    "\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "#CRIANDO O MODELO COM EXPANS√ÉO POLINOMIAL\n",
    "poly_expansion = PolynomialExpansion(\n",
    "    inputCol=\"features_raw\", \n",
    "    outputCol=\"poly_features\", \n",
    "    degree=2\n",
    ")\n",
    "\n",
    "# ATUALIZANDO O SCALER\n",
    "scaler_poly = StandardScaler(\n",
    "    inputCol=\"poly_features\", \n",
    "    outputCol=\"features_scaled\",\n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "\n",
    "# PIPELINE N√ÉO-LINEAR\n",
    "pipeline_nlinear = Pipeline(stages=[\n",
    "    assembler,       \n",
    "    poly_expansion,  \n",
    "    scaler_poly,    \n",
    "    svm           \n",
    "])\n",
    "\n",
    "# Precisamos atualizar a coluna de features do SVM para ler \"features_scaled\"\n",
    "svm.setFeaturesCol(\"features_scaled\")\n",
    "\n",
    "# TREINAMENTO E AVALIA√á√ÉO COMPARATIVA\n",
    "print(\"‚è≥ Treinando SVM com Expans√£o Polinomial (Grau 2)...\")\n",
    "model_nlinear = pipeline_nlinear.fit(train_data)\n",
    "predictions_nlinear = model_nlinear.transform(test_data)\n",
    "\n",
    "# Avalia√ß√£o\n",
    "acc_nlinear = evaluator_acc.evaluate(predictions_nlinear)\n",
    "f1_nlinear = evaluator_f1.evaluate(predictions_nlinear)\n",
    "\n",
    "# C√°lculo do recall para a classe 'Maligno' (1)\n",
    "TP_nlinear = predictions_nlinear.filter((F.col(\"label\") == 1) & (F.col(\"prediction\") == 1)).count()\n",
    "FN_nlinear = predictions_nlinear.filter((F.col(\"label\") == 1) & (F.col(\"prediction\") == 0)).count()\n",
    "recall_nlinear = TP_nlinear / (TP_nlinear + FN_nlinear) if (TP_nlinear + FN_nlinear) > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"üìä RESULTADO FINAL (COMPARATIVO)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"1. SVM Simples (Linear):     {accuracy:.2%}\")\n",
    "print(f\"2. SVM Polinomial (Grau 2):  {acc_nlinear:.2%}\")\n",
    "print(f\"‚öñÔ∏è  F1-Score (Equil√≠brio) Polinomial: {f1_nlinear:.2%}\")\n",
    "print(f\"üö® Recall (Sensibilidade - Maligno) Polinomial: {recall_nlinear:.2%}\")\n",
    "print(\"\\n--- Matriz de Confus√£o ---\")\n",
    "confusion_matrix_nlinear = predictions_nlinear.groupBy(\"label\", \"prediction\").count() \\\n",
    "                              .orderBy(\"label\", \"prediction\")\n",
    "\n",
    "display(confusion_matrix_nlinear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb8dbb0-c9d7-4c0a-b08d-a3a01c89f4ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# O MODELO COM (SCIKIT-LEARN E KERNEL RBF) ---\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score\n",
    "\n",
    "print(\"üîÑ Convertendo dados do Spark para Pandas (Para usar Scikit-Learn)...\")\n",
    "df_pandas = df_clean.toPandas()\n",
    "\n",
    "# 1. Prepara√ß√£o ao modo scikit-learn\n",
    "X = df_pandas.drop(\"label\", axis=1)\n",
    "y = df_pandas[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler_sklearn = StandardScaler()\n",
    "X_train_scaled = scaler_sklearn.fit_transform(X_train)\n",
    "X_test_scaled = scaler_sklearn.transform(X_test)\n",
    "\n",
    "# Treinando o RBF \n",
    "svm_rbf = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=42)\n",
    "svm_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Avalia√ß√£o\n",
    "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
    "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "f1_rbf = f1_score(y_test, y_pred_rbf)\n",
    "recall_rbf = recall_score(y_test, y_pred_rbf)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"üèÜ PLACAR FINAL DOS MODELOS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"1. Spark Linear:        95.10%\")\n",
    "print(f\"2. Spark Polinomial:    97.90%\")\n",
    "print(f\"3. Scikit Kernel RBF:   {acc_rbf:.2%}\")\n",
    "print(f\"‚öñÔ∏è  F1-Score Kernel RBF:           {f1_rbf:.2%}\")\n",
    "print(f\"üö® Recall (Maligno) RBF:    {recall_rbf:.2%}\")\n",
    "print(\"-\" * 40)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "breast_cancer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
